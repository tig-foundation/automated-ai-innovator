version: '3.8'

services:
  vllm-server:
    image: vllm/vllm-openai:v0.9.2
    ports:  
      - "8000:8000"
    environment:
      #- CUDA_VISIBLE_DEVICES=0
      - HUGGING_FACE_HUB_TOKEN=hf_
      #- PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32
      #- CUDA_LAUNCH_BLOCKING=1
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: >
      --model mistralai/Mistral-7B-Instruct-v0.2
      --host 0.0.0.0
      --port 8000
      --quantization bitsandbytes
      --load-format bitsandbytes
      --max-num-seqs 32
      --gpu-memory-utilization 0.8
      --max-model-len 8192
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  open-responses-server:
    image: ghcr.io/teabranch/open-responses-server:sha-fb5c0d6
    ports:
      - "8080:8080"
    environment:
      - OPENAI_BASE_URL_INTERNAL=http://vllm-server:8000
      - OPENAI_BASE_URL=http://localhost:8080
      - OPENAI_API_KEY=sk-mockapikey123456789
      - API_ADAPTER_HOST=0.0.0.0
      - API_ADAPTER_PORT=8080
    command: >
      sh -c "
      echo '{\"sqlite\": {\"command\": \"uvx\", \"args\": [\"mcp-server-sqlite\", \"--db-path\", \"/tmp/test.db\"]}}' > /tmp/servers_config.json &&
      export MCP_SERVERS_CONFIG_PATH=/tmp/servers_config.json &&
      uvicorn open_responses_server.server_entrypoint:app --host 0.0.0.0 --port 8080
      "
    depends_on:
      - vllm-server
    
  jupyter:
    image: jupyter/scipy-notebook:latest
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=autoinnovator
    volumes:
      - ./automated-ai-innovator:/home/jovyan/work/automated-ai-innovator
      - jupyter_data:/home/jovyan/.jupyter
    working_dir: /home/jovyan/work/automated-ai-innovator
    command: >
      bash -c "
      pip install -r requirements.txt &&
      start-notebook.sh --NotebookApp.token='autoinnovator' --NotebookApp.password='' --NotebookApp.allow_root=True --ip='0.0.0.0'
      "
    depends_on:
      - open-responses-server

volumes:
  vllm_models:
  jupyter_data:


# curl -X POST http://localhost:8080/responses \
#   -H "Content-Type: application/json" \
#   -H "Authorization: Bearer sk-mockapikey123456789" \
#   -d '{
#     "model": "mistralai/Mistral-7B-Instruct-v0.2",
#     "stream": true,
#     "input": [
#       {
#         "type": "message",
#         "role": "user",
#         "content": [
#           {
#             "type": "input_text",
#             "text": "Tell me a joke"
#           }
#         ]
#       }
#     ]
#   }'