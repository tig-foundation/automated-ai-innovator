{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1050b51-5f6c-4bf9-a6ac-a71722551513",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791bf8e6-a42f-43b3-9bb1-3d73cd29ca2d",
   "metadata": {},
   "source": [
    "## Step 1: Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3427ebda-6065-440b-93e8-dd0c5d63c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d027c72-e530-4b90-b669-95ec6b8fd5e2",
   "metadata": {},
   "source": [
    "### Step 2: Define Your AutoInnovator\n",
    "\n",
    "An **AutoInnovator** operates in a loop, where each iteration—called a **generation**—produces a set of **candidate algorithms**. The process for each candidate is:\n",
    "\n",
    "1. **Create Prompt** – Your defined `create_prompt_kwargs` is used to construct a request to send to a Large Language Model (LLM)\n",
    "   * See https://platform.openai.com/docs/api-reference/responses/create for what can be included in your request\n",
    "   * See `Context` and `Candidate` classes in [autoinnovator/framework.py](autoinnovator/framework.py) for what historic data you can access\n",
    "3. **Send Prompt to LLM** – The prompt is submitted to a LLM.\n",
    "4. **Extract Algorithm Code** – Your defined `extract_algorithm_code` is used to extract algorithm code from the LLM’s response.\n",
    "5. **Evaluate Algorithm** – The candidate algorithm is tested and scored.\n",
    "\n",
    "Each candidate is generated independently but has access to the full history of prior generations. The goal is to design your AutoInnovator in a way that uses data from previous generations to guide the creation of higher-performing algorithms over generations.\n",
    "\n",
    "> ⚠️ **Important:** The AutoInnovator should not directly edit or mutate algorithms. Instead, it evolves its approach by refining prompts and strategies for generating new candidates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c0251-e6e1-4654-84ea-5150f1c35ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoinnovator import AutoInnovatorBase, Candidate, Context\n",
    "import os\n",
    "\n",
    "class SimpleAutoInnovator(AutoInnovatorBase):\n",
    "    def system_prompt(self, ctx: Context) -> str:\n",
    "        with open(os.path.join(\"example_prompts\", f\"{ctx.challenge.name}.txt\"), \"r\") as f:\n",
    "            prompt = f.read()\n",
    "        return prompt.format(\n",
    "            parameters=ctx.challenge.parameters,\n",
    "            base_algorithm=ctx.challenge.base_algorithm\n",
    "        )\n",
    "\n",
    "    def create_prompt_kwargs(self, candidate: Candidate, ctx: Context) -> dict:\n",
    "        if candidate.generation == 1:\n",
    "            prev_candidate = ctx.candidates[0][0] # 0th gen just has 1 candidate using base algorithm\n",
    "        else:\n",
    "            prev_candidate = ctx.candidates[candidate.generation - 1][candidate.id] # just use 1 candidate from previous generation\n",
    "        if not prev_candidate.success:\n",
    "            # Another strategy would be to use the previous generation's best candidate\n",
    "            raise RuntimeError(\"Previous candidate was not successful. Simple Innovator strategy cannot continue.\")\n",
    "        algorithm = prev_candidate.algorithm\n",
    "        response = prev_candidate.response\n",
    "        if response:\n",
    "            prev_response_id = response[\"id\"]\n",
    "            text = response[\"output\"][0][\"content\"][0][\"text\"]\n",
    "            reasoning = text.split('<reasoning>')[1].split('</reasoning>')[0]\n",
    "        else:\n",
    "            prev_response_id = None\n",
    "            reasoning = \"No reasoning yet. Evaluation was on base algorithm.\"\n",
    "        evaluation = prev_candidate.evaluation\n",
    "        evaluation[\"algorithm_code_length\"] = len(algorithm)\n",
    "        \n",
    "        return {\n",
    "            \"instructions\": self.system_prompt(ctx),\n",
    "            \"input\": f\"ALGORITHM:\\n{algorithm}\\nREASONING:\\n{reasoning}\\nEVALUATION:\\n{evaluation}\",\n",
    "            \"temperature\": 1.0,\n",
    "            \"previous_response_id\": prev_response_id,\n",
    "        }\n",
    "\n",
    "    def extract_algorithm_code(self, response: dict) -> str:\n",
    "        text = response[\"output\"][0][\"content\"][0][\"text\"]\n",
    "        if \"<python>\" in text and \"</python>\" in text:\n",
    "            return text.split(\"<python>\")[1].split(\"</python>\")[0].strip()\n",
    "        raise ValueError(\"Response does not contain valid algorithm code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b29408c-70bb-4d08-9b56-7b833793e608",
   "metadata": {},
   "source": [
    "## Step 3: Run Your AutoInnovator on the Challenges\n",
    "\n",
    "We’ve provided **three built-in challenges** to test your AutoInnovator:\n",
    "\n",
    "1. **KDE** – Generate parameters for a Gaussian Mixture Model that best fits samples drawn from an intractable target distribution.\n",
    "2. **Binning** – Generate histogram bin edges within the interval $[-1, 1]$ that best represent samples from an intractable target distribution over $[-1, 1]$.\n",
    "3. **Clustering** – Assign samples from an intractable target distribution into $K$ clusters (where $K$ is provided) to maximize the **pairwise cluster connectivity score**.\n",
    "\n",
    "> ⚠️ You should not modify the challenges themselves when developing your AutoInnovator. However, you’re welcome to inspect the challenge code to better understand the setup.\n",
    "\n",
    "### Challenge Code Structure\n",
    "\n",
    "Each challenge is organized into the following components:\n",
    "1. `challenges/<challenge_name>.py` → Defines the challenge as an executable Python script.\n",
    "2. `challenges/configs/<challenge_name>.json` → Specifies configuration settings for generating challenge instances.  Each candidate is evaluated against the same fixed set of instances.\n",
    "3. `challenges/base_algos/<challenge_name>_<algo_name>.py` → Provides a baseline algorithm implementation for solving challenge instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c35d7-00d0-4389-b01d-28d615341aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = None\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"You must set an OpenAI API Key\")\n",
    "\n",
    "from autoinnovator import Challenge, LLM, LLMProvider\n",
    "llm = LLM(\n",
    "    provider=LLMProvider.OPENAI,\n",
    "    model=\"gpt-4.1-mini-2025-04-14\",\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "my_autoinnovator = SimpleAutoInnovator()\n",
    "kde_challenge = Challenge(\"kde\")\n",
    "binning_challenge = Challenge(\"binning\")\n",
    "clustering_challenge = Challenge(\"clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe970878-7c7c-4fab-bb58-f2b40ce11d69",
   "metadata": {},
   "source": [
    "### 3.1 Running KDE Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c269788-e2cd-4ecb-a74d-733298c5ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, clear_output, Markdown\n",
    "import time\n",
    "\n",
    "num_visualisations = 4\n",
    "def on_generation_done(ctx: Context):\n",
    "    clear_output(wait=True)\n",
    "    best_candidate = min(\n",
    "        filter(lambda c: c.success, ctx.candidates[ctx.curr_generation]),\n",
    "        key=lambda c: c.evaluation[\"test_log_likelihood_average\"]\n",
    "    )\n",
    "    display(Markdown(f\"### Generation {ctx.curr_generation} - Best Candidate {best_candidate.id}\"))\n",
    "    for i in range(num_visualisations):\n",
    "        display(Image(filename=best_candidate.visualisation_path.format(i=i)))\n",
    "    display(Image(filename=ctx.results_plot_path))\n",
    "    time.sleep(1)\n",
    "\n",
    "kde_ctx = my_autoinnovator.run(\n",
    "    llm=llm,\n",
    "    challenge=kde_challenge,\n",
    "    num_generations=10,\n",
    "    candidates_per_generation=1,\n",
    "    experiment_dir=os.path.join(\"experiments\", \"kde\"),\n",
    "    num_visualisations_per_candidate=num_visualisations,\n",
    "    on_generation_done=on_generation_done\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f76a35c-8ba7-4112-87da-e70f02fd0a30",
   "metadata": {},
   "source": [
    "### 3.2 Running Binning Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba18aa-1857-4926-88b7-73d2e4b33a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, clear_output, Markdown\n",
    "import time\n",
    "\n",
    "num_visualisations = 4\n",
    "def on_generation_done(ctx: Context):\n",
    "    clear_output(wait=True)\n",
    "    best_candidate = min(\n",
    "        filter(lambda c: c.success, ctx.candidates[ctx.curr_generation]),\n",
    "        key=lambda c: c.evaluation[\"test_log_likelihood_average\"]\n",
    "    )\n",
    "    display(Markdown(f\"### Generation {ctx.curr_generation} - Best Candidate {best_candidate.id}\"))\n",
    "    for i in range(num_visualisations):\n",
    "        display(Image(filename=best_candidate.visualisation_path.format(i=i)))\n",
    "    display(Image(filename=ctx.results_plot_path))\n",
    "    time.sleep(1)\n",
    "\n",
    "binning_ctx = my_autoinnovator.run(\n",
    "    llm=llm,\n",
    "    challenge=binning_challenge,\n",
    "    num_generations=10,\n",
    "    candidates_per_generation=1,\n",
    "    experiment_dir=os.path.join(\"experiments\", \"binning\"),\n",
    "    num_visualisations_per_candidate=num_visualisations,\n",
    "    on_generation_done=on_generation_done\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f64b98-b126-4704-80d3-5c93a45e3837",
   "metadata": {},
   "source": [
    "### 3.3 Running Clustering Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06eb254-6271-451a-95a8-f2fe5873bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, clear_output, Markdown\n",
    "import time\n",
    "\n",
    "num_visualisations = 4\n",
    "def on_generation_done(ctx: Context):\n",
    "    clear_output(wait=True)\n",
    "    best_candidate = min(\n",
    "        filter(lambda c: c.success, ctx.candidates[ctx.curr_generation]),\n",
    "        key=lambda c: c.evaluation[\"cluster_scores_average\"]\n",
    "    )\n",
    "    display(Markdown(f\"### Generation {ctx.curr_generation} - Best Candidate {best_candidate.id}\"))\n",
    "    for i in range(num_visualisations):\n",
    "        display(Image(filename=best_candidate.visualisation_path.format(i=i)))\n",
    "    display(Image(filename=ctx.results_plot_path))\n",
    "    time.sleep(1)\n",
    "\n",
    "clustering_ctx = my_autoinnovator.run(\n",
    "    llm=llm,\n",
    "    challenge=clustering_challenge,\n",
    "    num_generations=10,\n",
    "    candidates_per_generation=1,\n",
    "    experiment_dir=os.path.join(\"experiments\", \"clustering\"),\n",
    "    num_visualisations_per_candidate=num_visualisations,\n",
    "    on_generation_done=on_generation_done\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
